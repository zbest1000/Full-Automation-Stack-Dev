{
  "version": "2",
  "templates": [
    {
      "type": "stack",
      "title": "Industrial Automation Stack via Tailscale",
      "name": "industrial-automation-stack-tailscale",
      "description": "Complete industrial automation toolkit with Node-RED, FlowFuse, HiveMQ, Ignition, TimeBase, MonsterMQ, Grafana, InfluxDB, PostgreSQL, and Redis. All services are exposed via Tailscale sidecar and host ports for local access.",
      "note": "\u26a0\ufe0f IMPORTANT: Provide a valid Tailscale auth key (TS_AUTHKEY) before deploying. Services are accessible both via Tailscale serve mappings and host ports. The Portainer service is included for bare Docker hosts but MUST be removed/disabled when deploying from an existing Portainer instance. For direct Docker Compose deployment, use the docker-compose.yaml file in the repository root.",
      "categories": [
        "automation",
        "iot",
        "network",
        "monitoring",
        "industrial"
      ],
      "platform": "linux",
      "logo": "https://portainer-io-assets.s3.amazonaws.com/logos/portainer.png",
      "env": [
        {
          "name": "TS_AUTHKEY",
          "label": "Tailscale Auth Key",
          "description": "Reusable or ephemeral auth key that allows the stack to join your tailnet. Store securely.",
          "default": "",
          "required": true
        },
        {
          "name": "TAILSCALE_HOSTNAME",
          "label": "Tailscale Hostname",
          "description": "Hostname to register inside the tailnet.",
          "default": "automation-stack"
        },
        {
          "name": "FLOWFUSE_DOMAIN",
          "label": "FlowFuse Domain",
          "description": "Domain for FlowFuse Node-RED instance routing (e.g., automation-stack.ts.net or your own domain). Required for FlowFuse to route requests to Node-RED instances.",
          "default": "automation-stack.ts.net"
        },
        {
          "name": "POSTGRES_USER",
          "label": "PostgreSQL User",
          "description": "Database user for PostgreSQL service.",
          "default": "flowfuse"
        },
        {
          "name": "POSTGRES_PASSWORD",
          "label": "PostgreSQL Password",
          "description": "Database password for PostgreSQL service.",
          "default": ""
        },
        {
          "name": "POSTGRES_DB",
          "label": "PostgreSQL Database",
          "description": "Default database name for PostgreSQL service.",
          "default": "flowfuse"
        },
        {
          "name": "GRAFANA_ADMIN_USER",
          "label": "Grafana Admin User",
          "description": "Initial admin username for Grafana.",
          "default": "admin"
        },
        {
          "name": "GRAFANA_ADMIN_PASSWORD",
          "label": "Grafana Admin Password",
          "description": "Initial admin password for Grafana.",
          "default": ""
        },
        {
          "name": "INFLUXDB_ADMIN_USER",
          "label": "InfluxDB Admin User",
          "description": "Bootstrap admin user for InfluxDB 2.x.",
          "default": "admin"
        },
        {
          "name": "INFLUXDB_ADMIN_PASSWORD",
          "label": "InfluxDB Admin Password",
          "description": "Bootstrap admin password for InfluxDB 2.x.",
          "default": ""
        },
        {
          "name": "INFLUXDB_BUCKET",
          "label": "InfluxDB Bucket",
          "description": "Default bucket name created during bootstrap.",
          "default": "automation"
        },
        {
          "name": "INFLUXDB_ORG",
          "label": "InfluxDB Organization",
          "description": "Default organization name created during bootstrap.",
          "default": "automation"
        },
        {
          "name": "INFLUXDB_RETENTION",
          "label": "InfluxDB Retention (hours)",
          "description": "Retention policy duration (in hours) for the bootstrap bucket. 0 = infinite.",
          "default": "0"
        },
        {
          "name": "HIVEMQ_ALLOW_ANONYMOUS",
          "label": "HiveMQ Allow Anonymous",
          "description": "Enable anonymous MQTT connections (true/false).",
          "default": "true"
        },
        {
          "name": "HIVEMQ_USER",
          "label": "HiveMQ Username",
          "description": "Optional MQTT username for HiveMQ authentication.",
          "default": ""
        },
        {
          "name": "HIVEMQ_PASSWORD",
          "label": "HiveMQ Password",
          "description": "Optional MQTT password for HiveMQ authentication.",
          "default": ""
        }
      ],
      "composeFile": "version: '3.9'\nname: automation-stack\nservices:\n  tailscale:\n    image: tailscale/tailscale:stable\n    hostname: ${TAILSCALE_HOSTNAME}\n    environment:\n      TS_AUTHKEY: ${TS_AUTHKEY}\n    volumes:\n    - tailscale-state:/var/lib/tailscale\n    networks:\n    - automation\n    command:\n    - /bin/sh\n    - -c\n    - \"if [ -z \\\"${TS_AUTHKEY}\\\" ]; then\\n  echo 'ERROR: TS_AUTHKEY must be provided'\\\n      \\ >&2\\n  exit 1\\nfi\\ntailscaled --state=/var/lib/tailscale/tailscaled.state\\\n      \\ --socket=/tmp/tailscaled.sock --tun=userspace-networking &\\nTAILSCALED_PID=$!\\n\\\n      until tailscale --socket=/tmp/tailscaled.sock status >/dev/null 2>&1; do\\n \\\n      \\ sleep 0.5\\ndone\\ntailscale --socket=/tmp/tailscaled.sock up --authkey=${TS_AUTHKEY}\\\n      \\ --hostname=${TAILSCALE_HOSTNAME} --ssh\\ntailscale --socket=/tmp/tailscaled.sock\\\n      \\ serve reset\\ntailscale --socket=/tmp/tailscaled.sock serve --bg tcp 80 flowfuse:3000\\n\\\n      tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 1880 node-red-standalone:1880\\n\\\n      tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 1881 node-red-flowfuse-test:1880\\n\\\n      tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 1883 hivemq:1883\\ntailscale\\\n      \\ --socket=/tmp/tailscaled.sock serve --bg tcp 1884 hivemq-edge:1883\\ntailscale\\\n      \\ --socket=/tmp/tailscaled.sock serve --bg tcp 1885 monstermq:1883\\ntailscale\\\n      \\ --socket=/tmp/tailscaled.sock serve --bg tcp 8088 ignition:8088\\ntailscale\\\n      \\ --socket=/tmp/tailscaled.sock serve --bg tcp 8885 monstermq:8883\\ntailscale\\\n      \\ --socket=/tmp/tailscaled.sock serve --bg tcp 9000 monstermq:9000\\ntailscale\\\n      \\ --socket=/tmp/tailscaled.sock serve --bg tcp 9001 monstermq:9001\\ntailscale\\\n      \\ --socket=/tmp/tailscaled.sock serve --bg tcp 4840 monstermq:4840\\ntailscale\\\n      \\ --socket=/tmp/tailscaled.sock serve --bg tcp 4000 monstermq:4000\\ntailscale\\\n      \\ --socket=/tmp/tailscaled.sock serve --bg tcp 4511 historian:4511\\ntailscale\\\n      \\ --socket=/tmp/tailscaled.sock serve --bg tcp 4512 historian:4512\\ntailscale\\\n      \\ --socket=/tmp/tailscaled.sock serve --bg tcp 4531 explorer:4531\\ntailscale\\\n      \\ --socket=/tmp/tailscaled.sock serve --bg tcp 4532 explorer:4532\\ntailscale\\\n      \\ --socket=/tmp/tailscaled.sock serve --bg tcp 4521 simulator:4521\\ntailscale\\\n      \\ --socket=/tmp/tailscaled.sock serve --bg tcp 4522 simulator:4522\\ntailscale\\\n      \\ --socket=/tmp/tailscaled.sock serve --bg tcp 4523 opcua:4521\\ntailscale --socket=/tmp/tailscaled.sock\\\n      \\ serve --bg tcp 4524 opcua:4522\\ntailscale --socket=/tmp/tailscaled.sock serve\\\n      \\ --bg tcp 4525 mqtt:4521\\ntailscale --socket=/tmp/tailscaled.sock serve --bg\\\n      \\ tcp 4526 mqtt:4522\\ntailscale --socket=/tmp/tailscaled.sock serve --bg tcp\\\n      \\ 4527 sparkplugb:4521\\ntailscale --socket=/tmp/tailscaled.sock serve --bg tcp\\\n      \\ 4528 sparkplugb:4522\\ntailscale --socket=/tmp/tailscaled.sock serve --bg tcp\\\n      \\ 9086 influxdb:8086\\ntailscale --socket=/tmp/tailscaled.sock serve --bg tcp\\\n      \\ 9090 grafana:3000\\ntailscale --socket=/tmp/tailscaled.sock serve --bg tcp\\\n      \\ 5432 postgres:5432\\ntailscale --socket=/tmp/tailscaled.sock serve --bg tcp\\\n      \\ 9443 portainer:9443\\nwait $TAILSCALED_PID\\n\"\n    restart: unless-stopped\n    healthcheck:\n      test:\n      - CMD\n      - tailscale\n      - --socket=/tmp/tailscaled.sock\n      - status\n      - --json\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  postgres:\n    image: postgres:15-alpine\n    environment:\n      POSTGRES_USER: ${POSTGRES_USER}\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n      POSTGRES_DB: ${POSTGRES_DB}\n    volumes:\n    - postgres-data:/var/lib/postgresql/data\n    ports:\n    - 5432:5432\n    networks:\n    - automation\n    restart: unless-stopped\n    healthcheck:\n      test:\n      - CMD-SHELL\n      - pg_isready -U ${POSTGRES_USER}\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  redis:\n    image: redis:7-alpine\n    command: redis-server --save 60 1 --loglevel warning\n    volumes:\n    - redis-data:/data\n    networks:\n    - automation\n    restart: unless-stopped\n    healthcheck:\n      test:\n      - CMD\n      - redis-cli\n      - ping\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  flowfuse:\n    image: flowfuse/flowfuse:latest\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n    environment:\n      FF_PROJECTS_DIR: /var/lib/flowfuse\n      FF_STORAGE__TYPE: localfs\n      FF_DB__CLIENT: pg\n      FF_DB__PG__HOST: postgres\n      FF_DB__PG__USER: ${POSTGRES_USER}\n      FF_DB__PG__PASSWORD: ${POSTGRES_PASSWORD}\n      FF_DB__PG__DATABASE: ${POSTGRES_DB}\n      FF_REDIS__HOST: redis\n      DOMAIN: ${FLOWFUSE_DOMAIN:-${TAILSCALE_HOSTNAME}.ts.net}\n      FF_DRIVER__TYPE: docker\n      FF_DRIVER__OPTIONS__SOCKET: /var/run/docker.sock\n    volumes:\n    - flowfuse-data:/var/lib/flowfuse\n    - /var/run/docker.sock:/var/run/docker.sock:ro\n    ports:\n    - 3000:3000\n    networks:\n    - automation\n    restart: unless-stopped\n    healthcheck:\n      test:\n      - CMD-SHELL\n      - wget -qO- http://localhost:3000/ || exit 1\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  node-red:\n    image: nodered/node-red:3.1\n    hostname: node-red-standalone\n    container_name: node-red-standalone\n    environment:\n      NODE_RED_ENABLE_PROJECTS: 'true'\n    volumes:\n    - node-red-data:/data\n    ports:\n    - 1880:1880\n    networks:\n    - automation\n    restart: unless-stopped\n    healthcheck:\n      test:\n      - CMD-SHELL\n      - wget -qO- http://localhost:1880/ || exit 1\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  node-red-flowfuse-test:\n    image: nodered/node-red:3.1\n    hostname: node-red-flowfuse-test\n    container_name: node-red-flowfuse-test\n    environment:\n      NODE_RED_ENABLE_PROJECTS: 'true'\n    volumes:\n    - node-red-flowfuse-test-data:/data\n    ports:\n    - 1881:1880\n    networks:\n    - automation\n    restart: unless-stopped\n    healthcheck:\n      test:\n      - CMD-SHELL\n      - wget -qO- http://localhost:1880/ || exit 1\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  monstermq:\n    image: rocworks/monstermq:latest\n    hostname: monstermq\n    container_name: monstermq\n    depends_on:\n      postgres:\n        condition: service_healthy\n    ports:\n    - 1885:1883\n    - 8885:8883\n    - 9000:9000\n    - 9001:9001\n    - 4840:4840\n    - 4000:4000\n    restart: unless-stopped\n    volumes:\n    - monstermq-config:/app/config\n    - monstermq-data:/app/data\n    - monstermq-logs:/app/logs\n    networks:\n    - automation\n    healthcheck:\n      test:\n      - CMD-SHELL\n      - nc -z localhost 1883 || (wget -qO- http://localhost:4000/graphql 2>/dev/null\n        | grep -q 'graphql' && exit 0 || exit 1)\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  hivemq:\n    image: hivemq/hivemq-ce:latest\n    environment:\n      HIVEMQ_ALLOW_ANONYMOUS: ${HIVEMQ_ALLOW_ANONYMOUS}\n      HIVEMQ_USER: ${HIVEMQ_USER}\n      HIVEMQ_PASSWORD: ${HIVEMQ_PASSWORD}\n    volumes:\n    - hivemq-data:/opt/hivemq/data\n    ports:\n    - 1883:1883\n    networks:\n    - automation\n    restart: unless-stopped\n    healthcheck:\n      test:\n      - CMD-SHELL\n      - nc -z localhost 1883\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  hivemq-edge:\n    image: hivemq/hivemq-edge:latest\n    volumes:\n    - hivemq-edge-data:/opt/hivemq-edge/data\n    ports:\n    - 1884:1883\n    networks:\n    - automation\n    restart: unless-stopped\n    healthcheck:\n      test:\n      - CMD-SHELL\n      - nc -z localhost 1883\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  ignition:\n    image: kcollins/ignition:8.1\n    environment:\n      ACCEPT_EULA: Y\n    volumes:\n    - ignition-data:/usr/local/share/ignition/data\n    ports:\n    - 8088:8088\n    networks:\n    - automation\n    restart: unless-stopped\n    healthcheck:\n      test:\n      - CMD-SHELL\n      - nc -z localhost 8088 || (wget -qO- http://localhost:8088/main/system/health\n        2>/dev/null | grep -q 'health' && exit 0 || exit 1)\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  timebase-historian:\n    image: timebase/historian:latest\n    hostname: historian\n    container_name: historian\n    ports:\n    - 4511:4511\n    - 4512:4512\n    restart: unless-stopped\n    volumes:\n    - timebase-historian:/historian\n    environment:\n    - Settings=/historian/settings\n    - Data=/historian/data\n    - Logs=/historian/logs\n    networks:\n    - automation\n    healthcheck:\n      test:\n      - CMD-SHELL\n      - nc -z localhost 4511\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  timebase-explorer:\n    image: timebase/explorer:latest\n    hostname: explorer\n    container_name: explorer\n    ports:\n    - 4531:4531\n    - 4532:4532\n    restart: unless-stopped\n    volumes:\n    - timebase-explorer:/explorer\n    environment:\n    - Settings=/explorer/settings\n    - Config=/explorer/config\n    - Data=/explorer/data\n    - Logs=/explorer/logs\n    networks:\n    - automation\n    healthcheck:\n      test:\n      - CMD-SHELL\n      - nc -z localhost 4531\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  timebase-simulator:\n    image: timebase/collector:latest\n    hostname: simulator\n    container_name: simulator\n    ports:\n    - 4521:4521\n    - 4522:4522\n    restart: unless-stopped\n    volumes:\n    - timebase-simulator:/simulator\n    environment:\n    - Active=false\n    - Settings=/simulator/settings\n    - Config=/simulator/config\n    - Data=/simulator/data\n    - Logs=/simulator/logs\n    networks:\n    - automation\n    healthcheck:\n      test:\n      - CMD-SHELL\n      - nc -z localhost 4521\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  timebase-opcua:\n    image: timebase/collector:latest\n    hostname: opcua\n    container_name: opcua\n    ports:\n    - 4523:4521\n    - 4524:4522\n    restart: unless-stopped\n    volumes:\n    - timebase-opcua:/opcua\n    environment:\n    - Active=false\n    - Settings=/opcua/settings\n    - Config=/opcua/config\n    - Data=/opcua/data\n    - Logs=/opcua/logs\n    networks:\n    - automation\n    healthcheck:\n      test:\n      - CMD-SHELL\n      - nc -z localhost 4521\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  timebase-mqtt:\n    image: timebase/collector:latest\n    hostname: mqtt\n    container_name: mqtt\n    ports:\n    - 4525:4521\n    - 4526:4522\n    restart: unless-stopped\n    volumes:\n    - timebase-mqtt:/mqtt\n    environment:\n    - Active=false\n    - Settings=/mqtt/settings\n    - Config=/mqtt/config\n    - Data=/mqtt/data\n    - Logs=/mqtt/logs\n    networks:\n    - automation\n    healthcheck:\n      test:\n      - CMD-SHELL\n      - nc -z localhost 4521\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  timebase-sparkplugb:\n    image: timebase/collector:latest\n    hostname: sparkplugb\n    container_name: sparkplugb\n    ports:\n    - 4527:4521\n    - 4528:4522\n    restart: unless-stopped\n    volumes:\n    - timebase-sparkplugb:/sparkplugb\n    environment:\n    - Active=false\n    - Settings=/sparkplugb/settings\n    - Config=/sparkplugb/config\n    - Data=/sparkplugb/data\n    - Logs=/sparkplugb/logs\n    networks:\n    - automation\n    healthcheck:\n      test:\n      - CMD-SHELL\n      - nc -z localhost 4521\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  monstermq-config: null\n  monstermq-data: null\n  monstermq-logs: null\n  influxdb:\n    image: influxdb:2.7\n    environment:\n      DOCKER_INFLUXDB_INIT_MODE: setup\n      DOCKER_INFLUXDB_INIT_USERNAME: ${INFLUXDB_ADMIN_USER}\n      DOCKER_INFLUXDB_INIT_PASSWORD: ${INFLUXDB_ADMIN_PASSWORD}\n      DOCKER_INFLUXDB_INIT_ORG: ${INFLUXDB_ORG}\n      DOCKER_INFLUXDB_INIT_BUCKET: ${INFLUXDB_BUCKET}\n      DOCKER_INFLUXDB_INIT_RETENTION: ${INFLUXDB_RETENTION}\n    volumes:\n    - influxdb-data:/var/lib/influxdb2\n    - influxdb-config:/etc/influxdb2\n    ports:\n    - 9086:8086\n    networks:\n    - automation\n    restart: unless-stopped\n    healthcheck:\n      test:\n      - CMD-SHELL\n      - wget -qO- http://localhost:8086/health || exit 1\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  grafana:\n    image: grafana/grafana:10.4.3\n    environment:\n      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER}\n      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}\n    volumes:\n    - grafana-data:/var/lib/grafana\n    ports:\n    - 9090:3000\n    networks:\n    - automation\n    restart: unless-stopped\n    healthcheck:\n      test:\n      - CMD-SHELL\n      - wget -qO- http://localhost:3000/login || exit 1\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  portainer:\n    image: portainer/portainer-ce:latest\n    command: -H unix:///var/run/docker.sock\n    volumes:\n    - /var/run/docker.sock:/var/run/docker.sock\n    - portainer-data:/data\n    ports:\n    - 9443:9443\n    networks:\n    - automation\n    restart: unless-stopped\n    healthcheck:\n      test:\n      - CMD-SHELL\n      - wget -qO- http://localhost:9443/api/status || exit 1\n      interval: 5s\n      timeout: 3s\n      retries: 10\nnetworks:\n  automation:\n    driver: bridge\nvolumes:\n  tailscale-state: null\n  postgres-data: null\n  redis-data: null\n  flowfuse-data: null\n  node-red-data: null\n  node-red-flowfuse-test-data: null\n  monstermq-config: null\n  monstermq-log: null\n  monstermq-security: null\n  hivemq-data: null\n  hivemq-edge-data: null\n  ignition-data: null\n  influxdb-data: null\n  influxdb-config: null\n  grafana-data: null\n  portainer-data: null\n  timebase-historian: {}\n  timebase-explorer: {}\n  timebase-simulator: {}\n  timebase-opcua: {}\n  timebase-mqtt: {}\n  timebase-sparkplugb: {}\n"
    }
  ]
}