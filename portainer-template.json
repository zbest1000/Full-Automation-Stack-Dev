{
  "version": "2",
  "templates": [
    {
      "type": "stack",
      "title": "Industrial Automation Stack via Tailscale",
      "name": "industrial-automation-stack-tailscale",
      "description": "Complete industrial automation toolkit with Node-RED, FlowFuse, HiveMQ, Ignition, TimeBase, MonsterMQ, Grafana, InfluxDB, PostgreSQL, and Redis. All services are exposed via Tailscale sidecar and host ports for local access.",
      "note": "\u26a0\ufe0f IMPORTANT: Provide a valid Tailscale auth key (TS_AUTHKEY) before deploying. Services are accessible both via Tailscale serve mappings and host ports. The Portainer service is included for bare Docker hosts but MUST be removed/disabled when deploying from an existing Portainer instance. For direct Docker Compose deployment, use the docker-compose.yaml file in the repository root.",
      "categories": [
        "automation",
        "iot",
        "network",
        "monitoring",
        "industrial"
      ],
      "platform": "linux",
      "logo": "https://portainer-io-assets.s3.amazonaws.com/logos/portainer.png",
      "env": [
        {
          "name": "TS_AUTHKEY",
          "label": "Tailscale Auth Key",
          "description": "Reusable or ephemeral auth key that allows the stack to join your tailnet. Store securely.",
          "default": "",
          "required": true
        },
        {
          "name": "TAILSCALE_HOSTNAME",
          "label": "Tailscale Hostname",
          "description": "Hostname to register inside the tailnet.",
          "default": "automation-stack"
        },
        {
          "name": "POSTGRES_USER",
          "label": "PostgreSQL User",
          "description": "Database user for PostgreSQL service.",
          "default": "flowfuse"
        },
        {
          "name": "POSTGRES_PASSWORD",
          "label": "PostgreSQL Password",
          "description": "Database password for PostgreSQL service.",
          "default": ""
        },
        {
          "name": "POSTGRES_DB",
          "label": "PostgreSQL Database",
          "description": "Default database name for PostgreSQL service.",
          "default": "flowfuse"
        },
        {
          "name": "GRAFANA_ADMIN_USER",
          "label": "Grafana Admin User",
          "description": "Initial admin username for Grafana.",
          "default": "admin"
        },
        {
          "name": "GRAFANA_ADMIN_PASSWORD",
          "label": "Grafana Admin Password",
          "description": "Initial admin password for Grafana.",
          "default": ""
        },
        {
          "name": "INFLUXDB_ADMIN_USER",
          "label": "InfluxDB Admin User",
          "description": "Bootstrap admin user for InfluxDB 2.x.",
          "default": "admin"
        },
        {
          "name": "INFLUXDB_ADMIN_PASSWORD",
          "label": "InfluxDB Admin Password",
          "description": "Bootstrap admin password for InfluxDB 2.x.",
          "default": ""
        },
        {
          "name": "INFLUXDB_BUCKET",
          "label": "InfluxDB Bucket",
          "description": "Default bucket name created during bootstrap.",
          "default": "automation"
        },
        {
          "name": "INFLUXDB_ORG",
          "label": "InfluxDB Organization",
          "description": "Default organization name created during bootstrap.",
          "default": "automation"
        },
        {
          "name": "INFLUXDB_RETENTION",
          "label": "InfluxDB Retention (hours)",
          "description": "Retention policy duration (in hours) for the bootstrap bucket. 0 = infinite.",
          "default": "0"
        },
        {
          "name": "HIVEMQ_ALLOW_ANONYMOUS",
          "label": "HiveMQ Allow Anonymous",
          "description": "Enable anonymous MQTT connections (true/false).",
          "default": "true"
        },
        {
          "name": "HIVEMQ_USER",
          "label": "HiveMQ Username",
          "description": "Optional MQTT username for HiveMQ authentication.",
          "default": ""
        },
        {
          "name": "HIVEMQ_PASSWORD",
          "label": "HiveMQ Password",
          "description": "Optional MQTT password for HiveMQ authentication.",
          "default": ""
        }
      ],
      "composeFile": "version: '3.9'\nname: automation-stack\nservices:\n  tailscale:\n    image: tailscale/tailscale:stable\n    hostname: \"${TAILSCALE_HOSTNAME}\"\n    environment:\n      TS_AUTHKEY: ${TS_AUTHKEY}\n    volumes:\n      - tailscale-state:/var/lib/tailscale\n    networks:\n      - automation\n    command:\n      - /bin/sh\n      - -c\n      - |\n        if [ -z \"${TS_AUTHKEY}\" ]; then\n          echo 'ERROR: TS_AUTHKEY must be provided' >&2\n          exit 1\n        fi\n        tailscaled --state=/var/lib/tailscale/tailscaled.state --socket=/tmp/tailscaled.sock --tun=userspace-networking &\n        TAILSCALED_PID=$!\n        until tailscale --socket=/tmp/tailscaled.sock status >/dev/null 2>&1; do\n          sleep 0.5\n        done\n        tailscale --socket=/tmp/tailscaled.sock up --authkey=${TS_AUTHKEY} --hostname=${TAILSCALE_HOSTNAME} --ssh\n        tailscale --socket=/tmp/tailscaled.sock serve reset\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 80 flowfuse:3000\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 1880 node-red:1880\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 1883 hivemq:1883\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 1884 hivemq-edge:1883\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 1885 monstermq:1883\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 8088 ignition:8088\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 8885 monstermq:8883\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 9000 monstermq:9000\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 9001 monstermq:9001\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 4840 monstermq:4840\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 4000 monstermq:4000\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 4511 historian:4511\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 4512 historian:4512\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 4531 explorer:4531\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 4532 explorer:4532\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 4521 simulator:4521\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 4522 simulator:4522\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 4523 opcua:4521\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 4524 opcua:4522\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 4525 mqtt:4521\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 4526 mqtt:4522\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 4527 sparkplugb:4521\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 4528 sparkplugb:4522\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 9086 influxdb:8086\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 9090 grafana:3000\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 5432 postgres:5432\n        tailscale --socket=/tmp/tailscaled.sock serve --bg tcp 9443 portainer:9443\n        wait $TAILSCALED_PID\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD\", \"tailscale\", \"--socket=/tmp/tailscaled.sock\", \"status\", \"--json\"]\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  postgres:\n    image: postgres:15-alpine\n    environment:\n      POSTGRES_USER: ${POSTGRES_USER}\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n      POSTGRES_DB: ${POSTGRES_DB}\n    volumes:\n      - postgres-data:/var/lib/postgresql/data\n    ports:\n      - \"5432:5432\"\n\n    networks:\n      - automation\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U ${POSTGRES_USER}\"]\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  redis:\n    image: redis:7-alpine\n    command: redis-server --save 60 1 --loglevel warning\n    volumes:\n      - redis-data:/data\n    networks:\n      - automation\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  flowfuse:\n    image: flowfuse/flowfuse:latest\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n    environment:\n      FF_PROJECTS_DIR: /var/lib/flowfuse\n      FF_STORAGE__TYPE: localfs\n      FF_DB__CLIENT: pg\n      FF_DB__PG__HOST: postgres\n      FF_DB__PG__USER: ${POSTGRES_USER}\n      FF_DB__PG__PASSWORD: ${POSTGRES_PASSWORD}\n      FF_DB__PG__DATABASE: ${POSTGRES_DB}\n      FF_REDIS__HOST: redis\n    volumes:\n      - flowfuse-data:/var/lib/flowfuse\n    ports:\n      - \"3000:3000\"\n\n    networks:\n      - automation\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD-SHELL\", \"wget -qO- http://localhost:3000/ || exit 1\"]\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  node-red:\n    image: nodered/node-red:3.1\n    environment:\n      NODE_RED_ENABLE_PROJECTS: \"true\"\n    volumes:\n      - node-red-data:/data\n    ports:\n      - \"1880:1880\"\n\n    networks:\n      - automation\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD-SHELL\", \"wget -qO- http://localhost:1880/ || exit 1\"]\n      interval: 5s\n      timeout: 3s\n      retries: 10  monstermq:\n    image: rocworks/monstermq:latest\n    hostname: monstermq\n    container_name: monstermq\n    ports:\n      - \"1885:1883\"\n      - \"8885:8883\"\n      - \"9000:9000\"\n      - \"9001:9001\"\n      - \"4840:4840\"\n      - \"4000:4000\"\n    restart: unless-stopped\n    volumes:\n      - monstermq-config:/app/config\n      - monstermq-data:/app/data\n      - monstermq-logs:/app/logs\n    networks:\n      - automation\n    healthcheck:\n      test: [\"CMD-SHELL\", \"nc -z localhost 1883 || (wget -qO- http://localhost:4000/graphql 2>/dev/null | grep -q 'graphql' && exit 0 || exit 1)\"]\n      interval: 5s\n      timeout: 3s\n      retries: 10\n\n  hivemq:\n    image: hivemq/hivemq-ce:latest\n    environment:\n      HIVEMQ_ALLOW_ANONYMOUS: ${HIVEMQ_ALLOW_ANONYMOUS}\n      HIVEMQ_USER: ${HIVEMQ_USER}\n      HIVEMQ_PASSWORD: ${HIVEMQ_PASSWORD}\n    volumes:\n      - hivemq-data:/opt/hivemq/data\n    ports:\n      - \"1883:1883\"\n\n    networks:\n      - automation\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD-SHELL\", \"nc -z localhost 1883\"]\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  hivemq-edge:\n    image: hivemq/hivemq-edge:latest\n    volumes:\n      - hivemq-edge-data:/opt/hivemq-edge/data\n    ports:\n      - \"1884:1883\"\n\n    networks:\n      - automation\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD-SHELL\", \"nc -z localhost 1883\"]\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  ignition:\n    image: kcollins/ignition:8.1\n    environment:\n      ACCEPT_EULA: \"Y\"\n    volumes:\n      - ignition-data:/usr/local/share/ignition/data\n    ports:\n      - \"8088:8088\"\n\n    networks:\n      - automation\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD-SHELL\", \"nc -z localhost 8088 || (wget -qO- http://localhost:8088/main/system/health 2>/dev/null | grep -q 'health' && exit 0 || exit 1)\"]\n      interval: 5s\n      timeout: 3s\n      retries: 10\ninfluxdb:\n    image: influxdb:2.7\n    environment:\n      DOCKER_INFLUXDB_INIT_MODE: setup\n      DOCKER_INFLUXDB_INIT_USERNAME: ${INFLUXDB_ADMIN_USER}\n      DOCKER_INFLUXDB_INIT_PASSWORD: ${INFLUXDB_ADMIN_PASSWORD}\n      DOCKER_INFLUXDB_INIT_ORG: ${INFLUXDB_ORG}\n      DOCKER_INFLUXDB_INIT_BUCKET: ${INFLUXDB_BUCKET}\n      DOCKER_INFLUXDB_INIT_RETENTION: ${INFLUXDB_RETENTION}\n    volumes:\n      - influxdb-data:/var/lib/influxdb2\n      - influxdb-config:/etc/influxdb2\n    ports:\n      - \"9086:8086\"\n\n    networks:\n      - automation\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD-SHELL\", \"wget -qO- http://localhost:8086/health || exit 1\"]\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  grafana:\n    image: grafana/grafana:10.4.3\n    environment:\n      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER}\n      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}\n    volumes:\n      - grafana-data:/var/lib/grafana\n    ports:\n      - \"9090:3000\"\n\n    networks:\n      - automation\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD-SHELL\", \"wget -qO- http://localhost:3000/login || exit 1\"]\n      interval: 5s\n      timeout: 3s\n      retries: 10\n  portainer:\n    image: portainer/portainer-ce:latest\n    command: -H unix:///var/run/docker.sock\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - portainer-data:/data\n    ports:\n      - \"9443:9443\"\n\n    networks:\n      - automation\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD-SHELL\", \"wget -qO- http://localhost:9443/api/status || exit 1\"]\n      interval: 5s\n      timeout: 3s\n      retries: 10\nnetworks:\n  automation:\n    driver: bridge\nvolumes:\n  tailscale-state:\n  postgres-data:\n  redis-data:\n  flowfuse-data:\n  node-red-data:\n  monstermq-config:\n  monstermq-log:\n  monstermq-security:\n  hivemq-data:\n  hivemq-edge-data:\n  ignition-data:\n  influxdb-data:\n  influxdb-config:\n  grafana-data:\n  timebase-historian:\n    image: timebase/historian:latest\n    hostname: historian\n    container_name: historian\n    ports:\n      - \"4511:4511\"\n      - \"4512:4512\"\n    restart: unless-stopped\n    volumes:\n      - timebase-historian:/historian\n    environment:\n      - Settings=/historian/settings\n      - Data=/historian/data\n      - Logs=/historian/logs\n    networks:\n      - automation\n    healthcheck:\n      test: [\"CMD-SHELL\", \"nc -z localhost 4511\"]\n      interval: 5s\n      timeout: 3s\n      retries: 10\n\n  timebase-explorer:\n    image: timebase/explorer:latest\n    hostname: explorer\n    container_name: explorer\n    ports:\n      - \"4531:4531\"\n      - \"4532:4532\"\n    restart: unless-stopped\n    volumes:\n      - timebase-explorer:/explorer\n    environment:\n      - Settings=/explorer/settings\n      - Config=/explorer/config\n      - Data=/explorer/data\n      - Logs=/explorer/logs\n    networks:\n      - automation\n    healthcheck:\n      test: [\"CMD-SHELL\", \"nc -z localhost 4531\"]\n      interval: 5s\n      timeout: 3s\n      retries: 10\n\n  timebase-simulator:\n    image: timebase/collector:latest\n    hostname: simulator\n    container_name: simulator\n    ports:\n      - \"4521:4521\"\n      - \"4522:4522\"\n    restart: unless-stopped\n    volumes:\n      - timebase-simulator:/simulator\n    environment:\n      - Active=false\n      - Settings=/simulator/settings\n      - Config=/simulator/config\n      - Data=/simulator/data\n      - Logs=/simulator/logs\n    networks:\n      - automation\n    healthcheck:\n      test: [\"CMD-SHELL\", \"nc -z localhost 4521\"]\n      interval: 5s\n      timeout: 3s\n      retries: 10\n\n  timebase-opcua:\n    image: timebase/collector:latest\n    hostname: opcua\n    container_name: opcua\n    ports:\n      - \"4523:4521\"\n      - \"4524:4522\"\n    restart: unless-stopped\n    volumes:\n      - timebase-opcua:/opcua\n    environment:\n      - Active=false\n      - Settings=/opcua/settings\n      - Config=/opcua/config\n      - Data=/opcua/data\n      - Logs=/opcua/logs\n    networks:\n      - automation\n    healthcheck:\n      test: [\"CMD-SHELL\", \"nc -z localhost 4521\"]\n      interval: 5s\n      timeout: 3s\n      retries: 10\n\n  timebase-mqtt:\n    image: timebase/collector:latest\n    hostname: mqtt\n    container_name: mqtt\n    ports:\n      - \"4525:4521\"\n      - \"4526:4522\"\n    restart: unless-stopped\n    volumes:\n      - timebase-mqtt:/mqtt\n    environment:\n      - Active=false\n      - Settings=/mqtt/settings\n      - Config=/mqtt/config\n      - Data=/mqtt/data\n      - Logs=/mqtt/logs\n    networks:\n      - automation\n    healthcheck:\n      test: [\"CMD-SHELL\", \"nc -z localhost 4521\"]\n      interval: 5s\n      timeout: 3s\n      retries: 10\n\n  timebase-sparkplugb:\n    image: timebase/collector:latest\n    hostname: sparkplugb\n    container_name: sparkplugb\n    ports:\n      - \"4527:4521\"\n      - \"4528:4522\"\n    restart: unless-stopped\n    volumes:\n      - timebase-sparkplugb:/sparkplugb\n    environment:\n      - Active=false\n      - Settings=/sparkplugb/settings\n      - Config=/sparkplugb/config\n      - Data=/sparkplugb/data\n      - Logs=/sparkplugb/logs\n    networks:\n      - automation\n    healthcheck:\n      test: [\"CMD-SHELL\", \"nc -z localhost 4521\"]\n      interval: 5s\n      timeout: 3s\n      retries: 10\n\n  monstermq-config:\n  monstermq-data:\n  monstermq-logs:\n  portainer-data:\n"
    }
  ]
}